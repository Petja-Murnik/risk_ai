{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting customer churn\n",
    "\n",
    "Churn prediction--predicting whether a customer will stay or leave a company--is one of the more popular applications of machine learning for business, especially among consulting companies trying to sell their services.\n",
    "\n",
    "Typically the performance of a churn classifier (0 for customer stays, 1 for customer leaves, i.e. churns) is evaluated by a standard metric such as accuracy, precision, recall or ROC-AUC. In real-life, these metrics can be misleading, as they do not reflect the costs and benefits of the different outcomes being summarized by a given metric.\n",
    "\n",
    "In the case of churn, these costs and benefits can be made very explicit in terms of the classifier's confusion matrix, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html.\n",
    "\n",
    "\\begin{equation*}\n",
    "C =  \n",
    "\\begin{pmatrix}\n",
    "\\mathrm{true \\, positives} &  \\mathrm{false \\, positives}  \\\\\n",
    "\\mathrm{false \\, negatives} &  \\mathrm{true \\, negatives}\n",
    "\\end{pmatrix},\n",
    "\\end{equation*}\n",
    "\n",
    "or, more generally, for a classifer with $n$ outcomes, the entries of the confusion matrix $C = (C_{ij})$ are the counts of observations known to be in class $i$ and predicted to be in class $j$.\n",
    "\n",
    "To calculate a business-relevant metric, we need to know the cost for trying to retain a customer and the benefit of retaining a customer.\n",
    "\n",
    "## Standard metrics\n",
    "\n",
    "* Accuracy\n",
    "* Sensitivity (aka ``recall``)\n",
    "\n",
    "## Churn reward: simplest case\n",
    "\n",
    "The first case we consider is for a single action of sending customers an email. The reward is the revenue from the customer over the next year minux expenses per customer. Let's make the assumptions more explicit, and flag the ones that are reasonable or not as an approximation of reality.\n",
    "\n",
    "* action $a$ is defined by $a \\in (0,1) \\leftrightarrow (\\mathrm{no\\,email\\,sent}, \\mathrm{email\\,sent})$ has a fixed cost for all customers (reasonable),\n",
    "* there are no costs except the marketing action above (unreasonable)\n",
    "* revenue $\\mathrm{rev}$ is the same for all customers (unreasonable):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{rev} = \\begin{cases}\n",
    "0,  & \\text{if customer churns} \\\\\n",
    "\\mathrm{rev}_1, & \\text{if customer stays}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "The first task is to cast this setup as a *Markov Decision Process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from risk_learning.config import filenames\n",
    "from risk_learning.risk_learning import get_classifier_family_name\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filenames.fake_churn_simple)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split off test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at records per year for time split\n",
    "df.groupby('year').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_churn_data_target(df):\n",
    "    split_year = 2015\n",
    "    test = df.loc[df['year']>=split_year, :]\n",
    "    train_validate = df.loc[df['year'] < split_year]\n",
    "    \n",
    "    data_train_validate = train_validate[[c for c in df.columns if c != 'churn']]\n",
    "    lb = LabelBinarizer()\n",
    "    target_train_validate = lb.fit_transform(train_validate['churn']).ravel()\n",
    "\n",
    "    data_test = test[[c for c in df.columns if c != 'churn']]\n",
    "    target_test = lb.transform(test['churn']).ravel()\n",
    "    \n",
    "    return data_train_validate, target_train_validate, data_test, target_test\n",
    "\n",
    "\n",
    "data_train_validate, target_train_validate, data_test, target_test = split_churn_data_target(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put preprocessing and model selection in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnPipeline:\n",
    "    def __init__(self, data, target, mapper, test_size=0.25):\n",
    "        self._set_train_validate(data, target, test_size)\n",
    "        self.mapper = mapper\n",
    "        \n",
    "    def _set_train_validate(self, data, target, test_size):\n",
    "        X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "            data, target, test_size=0.25, random_state=42, stratify=target\n",
    "        )\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_validate = X_validate\n",
    "        self.y_validate = y_validate\n",
    "        \n",
    "    def hyperparameter_grid_select(self, clf_family_dict, param_grid):\n",
    "        family_name = clf_family_dict.get('name')\n",
    "        print('Hyperparameter fitting for {}'.format(family_name))\n",
    "        clf_family = clf_family_dict.get('clf')\n",
    "    \n",
    "        pipe = Pipeline([\n",
    "            ('featurize', self.mapper),\n",
    "            (family_name, clf_family)\n",
    "            ])\n",
    "\n",
    "        # Hyperparameter search\n",
    "        clf_select = GridSearchCV(pipe, param_grid, iid=False, cv=5, refit=True)\n",
    "        # Fit for cross validation folds across hyperparameter values\n",
    "        clf_select.fit(self.X_train, self.y_train)\n",
    "        print(\"Best parameter (CV score=%0.3f): {}\".format(clf_select.best_score_))\n",
    "        print(clf_select.best_params_)\n",
    "\n",
    "        return clf_select\n",
    "    \n",
    "    def clf_validation_score(self, clf):\n",
    "        print('\\nEvaluate score on validation set')\n",
    "        res = clf.score(self.X_validate, self.y_validate)\n",
    "        return res\n",
    "    \n",
    "    def clf_log_loss(self, clf):\n",
    "        print('\\nEvaluate log-loss on validation set')\n",
    "        res = log_loss(clf.predict(self.X_validate), self.y_validate)\n",
    "        return res\n",
    "    \n",
    "    def clf_confusion_matrix(self, clf):\n",
    "        print('\\nEvaluate confusion matrix on validation set')\n",
    "        res = confusion_matrix(self.y_validate, clf.predict(self.X_validate))\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "mapper = DataFrameMapper([\n",
    "    ('gender', LabelBinarizer()),\n",
    "    (['profession'], OneHotEncoder()), \n",
    "])\n",
    "\n",
    "print(\"Transformed features:\")\n",
    "pd.DataFrame(\n",
    "    mapper.fit_transform(data_train_validate),\n",
    "    columns=mapper.transformed_names_\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create churn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = ChurnPipeline(data_train_validate, target_train_validate, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "clf_family_dict = {\n",
    "    'name': 'lr',\n",
    "    'clf': LogisticRegression(solver='lbfgs', fit_intercept=True)\n",
    "}\n",
    "param_grid = {clf_family_dict.get('name') + '__C': np.logspace(1, 3, 20)}\n",
    "\n",
    "lr_clf = churn.hyperparameter_grid_select(clf_family_dict, param_grid)\n",
    "print(churn.clf_validation_score(lr_clf))\n",
    "print(churn.clf_log_loss(lr_clf))\n",
    "print(churn.clf_confusion_matrix(lr_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "clf_family_dict = {\n",
    "    'name': 'dt',\n",
    "    'clf': tree.DecisionTreeClassifier()\n",
    "}\n",
    "param_grid = {\n",
    "    clf_family_dict.get('name') + '__max_depth': range(1, 10, 1)\n",
    "}\n",
    "dt_clf = churn.hyperparameter_grid_select(clf_family_dict, param_grid)\n",
    "print(churn.clf_validation_score(dt_clf))\n",
    "print(churn.clf_log_loss(dt_clf))\n",
    "print(churn.clf_confusion_matrix(dt_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Trees\n",
    "clf_family_dict = {\n",
    "    'name': 'gbc',\n",
    "    'clf': GradientBoostingClassifier()\n",
    "}\n",
    "param_grid = {\n",
    "    clf_family_dict.get('name') + '__n_estimators': range(5, 10, 1)\n",
    "}\n",
    "\n",
    "gbc_clf = churn.hyperparameter_grid_select(clf_family_dict, param_grid)\n",
    "print(churn.clf_validation_score(gbc_clf))\n",
    "print(churn.clf_log_loss(gbc_clf))\n",
    "print(churn.clf_confusion_matrix(gbc_clf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated churn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filenames.fake_churn)\n",
    "data_train_validate, target_train_validate, data_test, target_test = split_churn_data_target(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "mapper = DataFrameMapper([\n",
    "    ('gender', LabelBinarizer()),\n",
    "    (['age'], StandardScaler()),\n",
    "    (['profession'], OneHotEncoder()), \n",
    "])\n",
    "\n",
    "churn = ChurnPipeline(data_train_validate, target_train_validate, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "clf_family_dict = {\n",
    "    'name': 'lr',\n",
    "    'clf': LogisticRegression(solver='lbfgs', fit_intercept=False)\n",
    "}\n",
    "param_grid = {clf_family_dict.get('name') + '__C': np.logspace(-4, 2, 20)}\n",
    "\n",
    "lr_clf = churn.hyperparameter_grid_select(clf_family_dict, param_grid)\n",
    "print(churn.clf_validation_score(lr_clf))\n",
    "print(churn.clf_log_loss(lr_clf))\n",
    "print(churn.clf_confusion_matrix(lr_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "clf_family_dict = {\n",
    "    'name': 'dt',\n",
    "    'clf': tree.DecisionTreeClassifier()\n",
    "}\n",
    "param_grid = {\n",
    "    clf_family_dict.get('name') + '__max_depth': range(1, 10, 1)\n",
    "}\n",
    "dt_clf = churn.hyperparameter_grid_select(clf_family_dict, param_grid)\n",
    "print(churn.clf_validation_score(dt_clf))\n",
    "print(churn.clf_log_loss(dt_clf))\n",
    "print(churn.clf_confusion_matrix(dt_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Trees\n",
    "clf_family_dict = {\n",
    "    'name': 'gbc',\n",
    "    'clf': GradientBoostingClassifier()\n",
    "}\n",
    "param_grid = {\n",
    "  #  clf_family_dict.get('name') + '__min_samples_leaf': range(3,10),\n",
    "    clf_family_dict.get('name') + '__n_estimators': range(5, 10, 1)\n",
    "}\n",
    "\n",
    "gbc_clf = churn.hyperparameter_grid_select(clf_family_dict, param_grid)\n",
    "print(churn.clf_validation_score(gbc_clf))\n",
    "print(churn.clf_log_loss(gbc_clf))\n",
    "print(churn.clf_confusion_matrix(gbc_clf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
